import torch
from torch import nn
import torch.nn.functional as F
from torch.nn import Module, ModuleList, Parameter, ParameterList


class MemoryMLP(Module):
    def __init__(self, dim, depth):
        super().__init__()
        self.weights = ParameterList(
            [Parameter(torch.randn(dim, dim)) for _ in range(depth)]
        )

        for weight in self.weights:
            nn.init.xavier_uniform_(weight)

    def forward(self, x):
        for ind, weight in enumerate(self.weights):
            is_first = ind == 0

            if not is_first:
                x = F.silu(x)

            x = x @ weight

        return x


# memory mlp, but with gated residual + final projection


class GatedResidualMemoryMLP(Module):
    def __init__(self, dim, depth, expansion_factor=2.0):
        super().__init__()
        dim_hidden = int(dim * expansion_factor)

        self.weights = ParameterList(
            [
                ParameterList(
                    [
                        Parameter(torch.randn(dim, dim_hidden)),
                        Parameter(torch.randn(dim_hidden, dim)),
                        Parameter(torch.randn(dim * 2, dim)),
                    ]
                )
                for _ in range(depth)
            ]
        )

        self.final_proj = Parameter(torch.randn(dim, dim))

        for param in self.parameters():
            nn.init.xavier_uniform_(param)

    def forward(self, x):
        for weight1, weight2, to_gates in self.weights:
            res = x

            hidden = x @ weight1
            hidden = F.silu(hidden)
            branch_out = hidden @ weight2

            # gated residual

            gates = cat((branch_out, res), dim=-1) @ to_gates
            x = res.lerp(branch_out, gates.sigmoid())

        return x @ self.final_proj


# memory mlp with factorized weights
# so can tradeoff capacity for smaller chunk sizes


class FactorizedMemoryMLP(Module):
    def __init__(self, dim, depth, k=32):
        super().__init__()
        self.weights = ParameterList(
            [
                ParameterList(
                    [
                        Parameter(torch.randn(dim, k)),
                        Parameter(torch.randn(k, dim)),
                    ]
                )
                for _ in range(depth)
            ]
        )

        for weight1, weight2 in self.weights:
            nn.init.xavier_uniform_(weight1)
            nn.init.xavier_uniform_(weight2)

    def forward(self, x):
        for ind, (weight1, weight2) in enumerate(self.weights):
            is_first = ind == 0

            if not is_first:
                x = F.silu(x)

            x = x @ weight1 @ weight2

        return x


# improvised attention as memory module


class MemoryAttention(Module):
    def __init__(self, dim, scale=8.0, expansion_factor=2.0):
        super().__init__()
        self.scale = scale
        dim_ff_hidden = int(dim * expansion_factor)

        self.weights = nn.ParameterList(
            [
                nn.Parameter(torch.randn(dim, dim)),  # queries
                nn.Parameter(torch.randn(dim, dim)),  # keys
                nn.Parameter(torch.randn(dim, dim)),  # values
                nn.Parameter(torch.randn(dim, dim_ff_hidden)),  # ff w1
                nn.Parameter(torch.randn(dim_ff_hidden, dim)),  # ff w2
            ]
        )

        for weight in self.weights:
            nn.init.xavier_uniform_(weight)

    def forward(self, x):
        wq, wk, wv, ffw1, ffw2 = self.weights

        q = F.normalize(x @ wq, dim=-1)
        k = F.normalize(x @ wk, dim=-1)
        v = x @ wv

        attn_out = F.scaled_dot_product_attention(
            q, k, v, scale=self.scale, is_causal=True
        )

        x = x + attn_out

        h = F.silu(x @ ffw1)
        out = h @ ffw2

        return out
